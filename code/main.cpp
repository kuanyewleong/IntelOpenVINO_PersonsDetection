/*
This is just the main cpp file

*/
#include <gflags/gflags.h>
#include <functional>
#include <iostream>
#include <fstream>
#include <random>
#include <memory>
#include <chrono>
#include <vector>
#include <string>
#include <algorithm>
#include <iterator>

#include <inference_engine.hpp>

#include <samples/common.hpp>
#include <samples/slog.hpp>

#include "object_detection_demo_ssd_async.hpp"
#include <ext_list.hpp>

#include <opencv2/opencv.hpp>

//using namespace InferenceEngine;


void frameToBlob(const cv::Mat& frame,
	InferenceEngine::InferRequest::Ptr& inferRequest,
                 const std::string& inputName) {
    if (FLAGS_auto_resize) {
        /* Just set input blob containing read image. Resize and layout conversion will be done automatically */
        inferRequest->SetBlob(inputName, wrapMat2Blob(frame));
    } else {
        /* Resize and copy data from the image to the input blob */
		InferenceEngine::Blob::Ptr frameBlob = inferRequest->GetBlob(inputName);
        matU8ToBlob<uint8_t>(frame, frameBlob);
    }
}

int main(int argc, char *argv[]) {	
	
	// --- for camera ---
	std::string camera_ID = argv[1];
	int int_camera_ID = stoi(camera_ID);
	cv::VideoCapture cap(int_camera_ID);
	// --- end for camera ---

	//cv::VideoCapture cap;
	//cap.open("D:\\Computer_Vision_Project\\VideoData_for_Testing\\ch01_20170901105335.avi"); 	
	//cap.open(argv[1]);

	const size_t width = (size_t)cap.get(cv::CAP_PROP_FRAME_WIDTH);
	const size_t height = (size_t)cap.get(cv::CAP_PROP_FRAME_HEIGHT);

	// Output Recording		
	/*cv::VideoWriter out("Intel_person-detection-retail-0013_vid3.mp4", cv::VideoWriter::fourcc('M', 'J', 'P', 'G'),
		cap.get(cv::CAP_PROP_FPS), cv::Size(cap.get(cv::CAP_PROP_FRAME_WIDTH), cap.get(cv::CAP_PROP_FRAME_HEIGHT)), true);*/

	// read input (video) frame
	cv::Mat curr_frame;  // cap >> curr_frame;
	cv::Mat next_frame;

	// get plugin from C:\Intel\computer_vision_sdk_2018.4.420\inference_engine\bin\intel64\Release
	// if it is not set in local environment path
	// the 3 dlls needed are: inference_engine.dll, mkl_tiny_omp.dll, MKLDNNPlugin.dll
	InferenceEngine::InferencePlugin plugin = InferenceEngine::PluginDispatcher({ "../../../lib/intel64", "" }).getPluginByDevice("CPU");
	plugin.AddExtension(std::make_shared<InferenceEngine::Extensions::Cpu::CpuExtensions>());
	// -----------------------------------------------------------------------------------------------------

	// --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
	slog::info << "Loading network files" << slog::endl;

	//std::string FLAGS_m = "D:\\Computer_Vision_Project\\IntelModels\\person-detection-retail-0013.xml";
	std::string FLAGS_m = "pedestrian-detection-adas-0002.xml";
	//std::string FLAGS_m = argv[2];
	InferenceEngine::CNNNetReader netReader;
	/** Read network model **/
	netReader.ReadNetwork(FLAGS_m);
	/** Set batch size to 1 **/	
	netReader.getNetwork().setBatchSize(1);
	/** Extract model name and load it's weights **/
	std::string binFileName = fileNameNoExt(FLAGS_m) + ".bin";
	netReader.ReadWeights(binFileName);
	
	// -----------------------------------------------------------------------------------------------------

	/** SSD-based network should have one input and one output **/
	// --------------------------- 3. Configure input & output ---------------------------------------------
	// --------------------------- Prepare input blobs -----------------------------------------------------
	InferenceEngine::InputsDataMap inputInfo(netReader.getNetwork().getInputsInfo());
	InferenceEngine::InputInfo::Ptr& input = inputInfo.begin()->second;
	std::string inputName = inputInfo.begin()->first;
	input->setPrecision(InferenceEngine::Precision::U8);
	if (FLAGS_auto_resize) {
		input->getPreProcess().setResizeAlgorithm(InferenceEngine::ResizeAlgorithm::RESIZE_BILINEAR);
		input->getInputData()->setLayout(InferenceEngine::Layout::NHWC);
	}
	else {
		input->getInputData()->setLayout(InferenceEngine::Layout::NCHW);
	}
	// --------------------------- Prepare output blobs -----------------------------------------------------	
	InferenceEngine::OutputsDataMap outputInfo(netReader.getNetwork().getOutputsInfo());
	InferenceEngine::DataPtr& output = outputInfo.begin()->second;
	std::string outputName = outputInfo.begin()->first;
	int num_classes = netReader.getNetwork().getLayerByName(outputName.c_str())->GetParamAsInt("num_classes");	
	InferenceEngine::SizeVector outputDims = output->getTensorDesc().getDims();
	int maxProposalCount = outputDims[2];
	int objectSize = outputDims[3];
	
	output->setPrecision(InferenceEngine::Precision::FP32);
	output->setLayout(InferenceEngine::Layout::NCHW);
	// -----------------------------------------------------------------------------------------------------

	// --------------------------- 4. Loading model to the plugin ------------------------------------------	
	InferenceEngine::ExecutableNetwork network = plugin.LoadNetwork(netReader.getNetwork(), {});
	// -----------------------------------------------------------------------------------------------------

	// --------------------------- 5. Create infer request -------------------------------------------------
	// InferenceEngine::InferRequest::Ptr async_infer_request_next = network.CreateInferRequestPtr();
	InferenceEngine::InferRequest::Ptr async_infer_request = network.CreateInferRequestPtr();
	// -----------------------------------------------------------------------------------------------------

	// --------------------------- 6. Do inference ---------------------------------------------------------
	
	// bool isLastFrame = false;
	
	typedef std::chrono::duration<double, std::ratio<1, 1000>> ms;
	auto total_t0 = std::chrono::high_resolution_clock::now();
	auto wallclock = std::chrono::high_resolution_clock::now();
	double ocv_decode_time = 0, ocv_render_time = 0;

	while (true) {
		auto t0 = std::chrono::high_resolution_clock::now();	

		cap >> curr_frame;
		frameToBlob(curr_frame, async_infer_request, inputName);

		auto t1 = std::chrono::high_resolution_clock::now();
		ocv_decode_time = std::chrono::duration_cast<ms>(t1 - t0).count();

		t0 = std::chrono::high_resolution_clock::now();
		// Main sync point:
		
		async_infer_request->StartAsync();

		if (InferenceEngine::OK == async_infer_request->Wait(InferenceEngine::IInferRequest::WaitMode::RESULT_READY)) {
			t1 = std::chrono::high_resolution_clock::now();
			ms detection = std::chrono::duration_cast<ms>(t1 - t0);

			t0 = std::chrono::high_resolution_clock::now();
			ms wall = std::chrono::duration_cast<ms>(t0 - wallclock);
			wallclock = t0;

			t0 = std::chrono::high_resolution_clock::now();
			std::ostringstream out;			
			out.str("");
			out << "Detection time  : " << std::fixed << std::setprecision(2) << detection.count()
				<< " ms ("
				<< 1000.f / detection.count() << " fps)";
			cv::putText(curr_frame, out.str(), cv::Point2f(0, 75), cv::FONT_HERSHEY_TRIPLEX, 0.6,
				cv::Scalar(0, 255, 0));

			// copyrights notice
			cv::putText(curr_frame, "copyrights @ kuanyewleong 2019", cv::Point2f(10, 98), cv::FONT_HERSHEY_TRIPLEX, 0.8,
				cv::Scalar(255, 255, 255));
			cv::putText(curr_frame, "This product is NOT for commercial usage!", cv::Point2f(10, 130), cv::FONT_HERSHEY_TRIPLEX, 0.8,
				cv::Scalar(255, 255, 255));
			cv::putText(curr_frame, "############### DEMO ONLY ##############", cv::Point2f(10, 160), cv::FONT_HERSHEY_TRIPLEX, 0.8,
				cv::Scalar(255, 255, 255));
			cv::putText(curr_frame, "############### DEMO ONLY ##############", cv::Point2f(10, 250), cv::FONT_HERSHEY_TRIPLEX, 0.8,
				cv::Scalar(255, 255, 255));
			cv::putText(curr_frame, "copyrights @ kuanyewleong 2019", cv::Point2f(10, 280), cv::FONT_HERSHEY_TRIPLEX, 0.8,
				cv::Scalar(255, 255, 255));

			// ---------------------------Process output blobs--------------------------------------------------
			// Processing results of the CURRENT request
			const float *detections = async_infer_request->GetBlob(outputName)->buffer().as<InferenceEngine::PrecisionTrait<InferenceEngine::Precision::FP32>::value_type*>();
			for (int i = 0; i < maxProposalCount; i++) {
				float image_id = detections[i * objectSize + 0];
				int label = static_cast<int>(detections[i * objectSize + 1]);
				float confidence = detections[i * objectSize + 2];
				float xmin = detections[i * objectSize + 3] * width;
				float ymin = detections[i * objectSize + 4] * height;
				float xmax = detections[i * objectSize + 5] * width;
				float ymax = detections[i * objectSize + 6] * height;

				if (image_id < 0) {
					std::cout << "Only " << i << " proposals found" << std::endl;
					break;
				}				

				if (confidence > FLAGS_t) {
					/** Drawing only objects when > confidence_threshold probability **/
					std::ostringstream conf;
					conf << std::setprecision(3) << confidence;
					cv::putText(curr_frame, conf.str(),
						cv::Point2f(xmin, ymin - 5), cv::FONT_HERSHEY_COMPLEX_SMALL, 1,
						cv::Scalar(255, 255, 255));
					cv::rectangle(curr_frame, cv::Point2f(xmin, ymin), cv::Point2f(xmax, ymax), cv::Scalar(250, 206, 135), 2);
				}
			}
		}
		cv::imshow("Detection results", curr_frame);
		// out.write(curr_frame);

		t1 = std::chrono::high_resolution_clock::now();
		ocv_render_time = std::chrono::duration_cast<ms>(t1 - t0).count();	

		const int key = cv::waitKey(1);
		if (27 == key)  // Esc
			break;		
	}
	// -----------------------------------------------------------------------------------------------------
	auto total_t1 = std::chrono::high_resolution_clock::now();
	ms total = std::chrono::duration_cast<ms>(total_t1 - total_t0);
	std::cout << "Total Inference time: " << total.count() << std::endl;

	/** Show performace results **/
	if (FLAGS_pc) {
		printPerformanceCounts(*async_infer_request, std::cout);
	}

	// out.release();
    return 0;
}
